{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8550b089",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "> Note: sometimes your answer doesn't match one of \n",
    "> the options exactly. That's fine. \n",
    "> Select the option that's closest to your solution.\n",
    "\n",
    "\n",
    "In this homework, we will use the lead scoring dataset. Download it from [here](https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv).\n",
    "\n",
    "\n",
    "In this dataset our desired target for classification task will be `converted` variable - has the client signed up to the platform or not. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ebe667",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget \"https://raw.githubusercontent.com/alexeygrigorev/datasets/master/course_lead_scoring.csv\" -O data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37afa5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c5d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4551424",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53452ce9",
   "metadata": {},
   "source": [
    "### Data preparation\n",
    "\n",
    "* Check if the missing values are presented in the features.\n",
    "* If there are missing values:\n",
    "    * For caterogiral features, replace them with 'NA'\n",
    "    * For numerical features, replace with with 0.0 \n",
    "\n",
    "\n",
    "Split the data into 3 parts: train/validation/test with 60%/20%/20% distribution. Use `train_test_split` function for that with `random_state=1`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd67a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ab1f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55b80cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = df.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical = df.select_dtypes(include=['int64', 'float64']).columns.tolist() \n",
    "numerical.remove('converted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83113a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical, numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ea46cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical] = df[numerical].fillna(0.0)\n",
    "df[categorical] = df[categorical].fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb5a61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\n",
    "df_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)\n",
    "df_test = df_test.reset_index(drop=True)\n",
    "len(df_train), len(df_val), len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89196c9",
   "metadata": {},
   "source": [
    "### Question 1: ROC AUC feature importance\n",
    "\n",
    "ROC AUC could also be used to evaluate feature importance of numerical variables. \n",
    "\n",
    "Let's do that\n",
    "\n",
    "* For each numerical variable, use it as score (aka prediction) and compute the AUC with the `y` variable as ground truth.\n",
    "* Use the training dataset for that\n",
    "\n",
    "\n",
    "If your AUC is < 0.5, invert this variable by putting \"-\" in front\n",
    "\n",
    "(e.g. `-df_train['balance']`)\n",
    "\n",
    "AUC can go below 0.5 if the variable is negatively correlated with the target variable. You can change the direction of the correlation by negating this variable - then negative correlation becomes positive.\n",
    "\n",
    "Which numerical variable (among the following 4) has the highest AUC?\n",
    "\n",
    "- `lead_score`\n",
    "- `number_of_courses_viewed`\n",
    "- `interaction_count`\n",
    "- `annual_income`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c39c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "for label in (numerical):\n",
    "    score = roc_auc_score(df['converted'],df[label])\n",
    "    if (score < 0.5):\n",
    "        print (\"recalculating for -\", label)\n",
    "        score = roc_auc_score(-df['converted'],df[label])\n",
    "    print(f'{label}: {score:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dc485c",
   "metadata": {},
   "source": [
    "Answer 1: number_of_courses_viewed has the highest AUC with 0.761"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa562167",
   "metadata": {},
   "source": [
    "### Question 2: Training the model\n",
    "\n",
    "Apply one-hot-encoding using `DictVectorizer` and train the logistic regression with these parameters:\n",
    "\n",
    "```python\n",
    "LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "```\n",
    "\n",
    "What's the AUC of this model on the validation dataset? (round to 3 digits)\n",
    "\n",
    "- 0.32\n",
    "- 0.52\n",
    "- 0.72\n",
    "- 0.92"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130e38af",
   "metadata": {},
   "outputs": [],
   "source": [
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "def train_model(df_train, dv):\n",
    "    train_dicts = df_train[categorical + numerical].to_dict(orient='records')\n",
    "    X_train = dv.fit_transform(train_dicts)\n",
    "    y_train = df_train['converted'].values\n",
    "    model = LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)\n",
    "    model.fit(X_train, y_train)\n",
    "    return model\n",
    "\n",
    "model = train_model(df_train, dv)\n",
    "\n",
    "def predict_val(df, dv, model):\n",
    "    dicts = df[categorical + numerical].to_dict(orient='records')\n",
    "    X = dv.transform(dicts)\n",
    "    y = df['converted'].values\n",
    "    y_pred = model.predict_proba(X)[:, 1]\n",
    "    return y, y_pred\n",
    "\n",
    "y_val, y_val_pred = predict_val(df_val, dv, model)\n",
    "\n",
    "print(round(roc_auc_score(y_val, (y_val_pred>=0.5)) , 3))\n",
    "\n",
    "\n",
    "print (y_val_pred[::10]>=0.5, y_val[::10])\n",
    "print(y_val.mean(), (y_val_pred>=0.5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a71a7dd",
   "metadata": {},
   "source": [
    "### Question 3: Precision and Recall\n",
    "\n",
    "Now let's compute precision and recall for our model.\n",
    "\n",
    "* Evaluate the model on all thresholds from 0.0 to 1.0 with step 0.01\n",
    "* For each threshold, compute precision and recall\n",
    "* Plot them\n",
    "\n",
    "At which threshold precision and recall curves intersect?\n",
    "\n",
    "* 0.145\n",
    "* 0.345\n",
    "* 0.545\n",
    "* 0.745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4531577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_precision_recall(y, y_pred, t=0.5):\n",
    "    actual_positive = (y == 1)\n",
    "    actual_negative = (y == 0)\n",
    "\n",
    "    predict_positive = (y_pred >= t)\n",
    "    predict_negative = (y_pred < t)\n",
    "\n",
    "    tp = (predict_positive & actual_positive).sum()\n",
    "    tn = (predict_negative & actual_negative).sum()\n",
    "\n",
    "    fp = (predict_positive & actual_negative).sum()\n",
    "    fn = (predict_negative & actual_positive).sum()\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    print (f'Threshold: {t:.2f}  Precision: {precision:.3f}  Recall: {recall:.3f}, TP: {tp} FP: {fp} TN: {tn} FN: {fn}')\n",
    "    return precision, recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e636cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 100)\n",
    "scores = []\n",
    "for t in thresholds:\n",
    "    precision, recall = get_precision_recall(y_val, y_val_pred, t)\n",
    "    scores.append((t, precision, recall))\n",
    "\n",
    "scores.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cec2bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot([x[0] for x in scores], [x[1] for x in scores], label='Precision')\n",
    "plt.plot([x[0] for x in scores], [x[2] for x in scores], label='Recall')\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dda263b",
   "metadata": {},
   "source": [
    "Answer 3: Precision and Recall seem to intersect around t=0.645 between .64 and .65\n",
    "\n",
    "Threshold: 0.63  Precision: 0.765  Recall: 0.819, TP: 140 FP: 43 TN: 79 FN: 31 \n",
    "\n",
    "Threshold: 0.64  Precision: 0.770  Recall: 0.784, TP: 134 FP: 40 TN: 82 FN: 37\n",
    "\n",
    "Threshold: 0.65  Precision: 0.778  Recall: 0.760, TP: 130 FP: 37 TN: 85 FN: 41\n",
    "\n",
    "Threshold: 0.66  Precision: 0.783  Recall: 0.737, TP: 126 FP: 35 TN: 87 FN: 45"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b2bb4b",
   "metadata": {},
   "source": [
    "### Question 4: F1 score\n",
    "\n",
    "Precision and recall are conflicting - when one grows, the other goes down. That's why they are often combined into the F1 score - a metrics that takes into account both\n",
    "\n",
    "This is the formula for computing F1:\n",
    "\n",
    "$$F_1 = 2 \\cdot \\cfrac{P \\cdot R}{P + R}$$\n",
    "\n",
    "Where $P$ is precision and $R$ is recall.\n",
    "\n",
    "Let's compute F1 for all thresholds from 0.0 to 1.0 with increment 0.01\n",
    "\n",
    "At which threshold F1 is maximal?\n",
    "\n",
    "- 0.14\n",
    "- 0.34\n",
    "- 0.54\n",
    "- 0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb90b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1scores = []\n",
    "for score in scores:\n",
    "    f1score = 2 * (score[1] * score[2]) / (score[1] + score[2])\n",
    "    f1scores.append([score[0],f1score])\n",
    "    print (score[0], f1score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([x[0] for x in f1scores], [x[1] for x in f1scores], label='F1 Score')\n",
    "plt.xlabel('Threshold')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed466d",
   "metadata": {},
   "source": [
    "### Question 5: 5-Fold CV\n",
    "\n",
    "\n",
    "Use the `KFold` class from Scikit-Learn to evaluate our model on 5 different folds:\n",
    "\n",
    "```\n",
    "KFold(n_splits=5, shuffle=True, random_state=1)\n",
    "```\n",
    "\n",
    "* Iterate over different folds of `df_full_train`\n",
    "* Split the data into train and validation\n",
    "* Train the model on train with these parameters: `LogisticRegression(solver='liblinear', C=1.0, max_iter=1000)`\n",
    "* Use AUC to evaluate the model on validation\n",
    "\n",
    "How large is standard deviation of the scores across different folds?\n",
    "\n",
    "- 0.0001\n",
    "- 0.006\n",
    "- 0.06\n",
    "- 0.36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f57abab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7004bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0b4ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=1)\n",
    "aucs = []\n",
    "dv = DictVectorizer(sparse=False)\n",
    "\n",
    "for train_index, val_index in tqdm(kf.split(df_full_train)):\n",
    "    df_train = df_full_train.iloc[train_index]\n",
    "    df_val = df_full_train.iloc[val_index]\n",
    "\n",
    "    model = train_model(df_train, dv)\n",
    "    y_val, y_val_pred = predict_val(df_val, dv, model)\n",
    "    auc = roc_auc_score(y_val, y_val_pred>=0.5)\n",
    "    aucs.append(auc)\n",
    "\n",
    "print(f'CV mean ROC AUC: {np.mean(aucs):.3f}, SD: {np.std(aucs):.3f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca41b67",
   "metadata": {},
   "source": [
    "### Question 6: Hyperparameter Tuning\n",
    "\n",
    "Now let's use 5-Fold cross-validation to find the best parameter `C`\n",
    "\n",
    "* Iterate over the following `C` values: `[0.000001, 0.001, 1]`\n",
    "* Initialize `KFold` with the same parameters as previously\n",
    "* Use these parameters for the model: `LogisticRegression(solver='liblinear', C=C, max_iter=1000)`\n",
    "* Compute the mean score as well as the std (round the mean and std to 3 decimal digits)\n",
    "\n",
    "Which `C` leads to the best mean score?\n",
    "\n",
    "- 0.000001\n",
    "- 0.001\n",
    "- 1\n",
    "\n",
    "If you have ties, select the score with the lowest std. If you still have ties, select the smallest `C`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afb62d8",
   "metadata": {},
   "source": [
    "## Submit the results\n",
    "\n",
    "* Submit your results here: https://courses.datatalks.club/ml-zoomcamp-2025/homework/hw04\n",
    "* If your answer doesn't match options exactly, select the closest one"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
