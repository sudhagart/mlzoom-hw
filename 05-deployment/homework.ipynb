{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09334429",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "> Note: sometimes your answer doesn't match one of the options exactly. \n",
    "> That's fine. \n",
    "> Select the option that's closest to your solution.\n",
    "> If it's exactly in between two options, select the higher value.\n",
    "\n",
    "We recommend using python 3.12 or 3.13 in this homework.\n",
    "\n",
    "In this homework, we're going to continue working with the lead scoring dataset. You don't need the dataset: we will provide the model for you.\n",
    "\n",
    "\n",
    "## Question 1\n",
    "\n",
    "* Install `uv`\n",
    "* What's the version of uv you installed?\n",
    "* Use `--version` to find out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b476a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Answer 1 \n",
      "uv 0.9.5\n"
     ]
    }
   ],
   "source": [
    "print (\"## Answer 1 \")\n",
    "!uv --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a48c4e",
   "metadata": {},
   "source": [
    "## Initialize an empty uv project\n",
    "\n",
    "You should create an empty folder for homework\n",
    "and do it there. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507e039c",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "* Use uv to install Scikit-Learn version 1.6.1 \n",
    "* What's the first hash for Scikit-Learn you get in the lock file?\n",
    "* Include the entire string starting with sha256:, don't include quotes\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38e866f",
   "metadata": {},
   "source": [
    "## Answer 2\n",
    "sha256:20e9e49ecd130598f1ca38a1d85090e1a600147b9c02fa6f15d69cb53d968fda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3779aa",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We have prepared a pipeline with a dictionary vectorizer and a model.\n",
    "\n",
    "It was trained (roughly) using this code:\n",
    "\n",
    "```python\n",
    "categorical = ['lead_source']\n",
    "numeric = ['number_of_courses_viewed', 'annual_income']\n",
    "\n",
    "df[categorical] = df[categorical].fillna('NA')\n",
    "df[numeric] = df[numeric].fillna(0)\n",
    "\n",
    "train_dict = df[categorical + numeric].to_dict(orient='records')\n",
    "\n",
    "pipeline = make_pipeline(\n",
    "    DictVectorizer(),\n",
    "    LogisticRegression(solver='liblinear')\n",
    ")\n",
    "\n",
    "pipeline.fit(train_dict, y_train)\n",
    "```\n",
    "\n",
    "> **Note**: You don't need to train the model. This code is just for your reference.\n",
    "\n",
    "And then saved with Pickle. Download it [here](https://github.com/DataTalksClub/machine-learning-zoomcamp/tree/master/cohorts/2025/05-deployment/pipeline_v1.bin).\n",
    "\n",
    "With `wget`:\n",
    "\n",
    "```bash\n",
    "wget https://github.com/DataTalksClub/machine-learning-zoomcamp/raw/refs/heads/master/cohorts/2025/05-deployment/pipeline_v1.bin\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2130d2c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-10-29 00:34:39--  https://github.com/DataTalksClub/machine-learning-zoomcamp/raw/refs/heads/master/cohorts/2025/05-deployment/pipeline_v1.bin\n",
      "Resolving github.com (github.com)... 140.82.116.3\n",
      "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/DataTalksClub/machine-learning-zoomcamp/refs/heads/master/cohorts/2025/05-deployment/pipeline_v1.bin [following]\n",
      "--2025-10-29 00:34:39--  https://raw.githubusercontent.com/DataTalksClub/machine-learning-zoomcamp/refs/heads/master/cohorts/2025/05-deployment/pipeline_v1.bin\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.108.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1300 (1.3K) [application/octet-stream]\n",
      "Saving to: ‘pipeline_v1.bin’\n",
      "\n",
      "pipeline_v1.bin     100%[===================>]   1.27K  --.-KB/s    in 0s      \n",
      "\n",
      "2025-10-29 00:34:40 (72.1 MB/s) - ‘pipeline_v1.bin’ saved [1300/1300]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/machine-learning-zoomcamp/raw/refs/heads/master/cohorts/2025/05-deployment/pipeline_v1.bin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a818b",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Let's use the model!\n",
    "\n",
    "* Write a script for loading the pipeline with pickle\n",
    "* Score this record:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"lead_source\": \"paid_ads\",\n",
    "    \"number_of_courses_viewed\": 2,\n",
    "    \"annual_income\": 79276.0\n",
    "}\n",
    "```\n",
    "\n",
    "What's the probability that this lead will convert? \n",
    "\n",
    "* 0.333\n",
    "* 0.533\n",
    "* 0.733\n",
    "* 0.933\n",
    "\n",
    "If you're getting errors when unpickling the files, check their checksum:\n",
    "\n",
    "```bash\n",
    "$ md5sum pipeline_v1.bin\n",
    "7d17d2e4dfbaf1e408e1a62e6e880d49 *pipeline_v1.bin\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4834eaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7d17d2e4dfbaf1e408e1a62e6e880d49  pipeline_v1.bin\n"
     ]
    }
   ],
   "source": [
    "!md5sum pipeline_v1.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f63b08d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline(steps=[('dictvectorizer', DictVectorizer()),\n",
      "                ('logisticregression', LogisticRegression(solver='liblinear'))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/base.py:440: InconsistentVersionWarning: Trying to unpickle estimator DictVectorizer from version 1.6.1 when using version 1.7.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/base.py:440: InconsistentVersionWarning: Trying to unpickle estimator LogisticRegression from version 1.6.1 when using version 1.7.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/home/codespace/.local/lib/python3.12/site-packages/sklearn/base.py:440: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.6.1 when using version 1.7.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "with open(\"pipeline_v1.bin\", \"rb\") as f_in:\n",
    "    pipeline = pickle.load(f_in)\n",
    "print(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87abc8e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.534])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record = {\n",
    "    \"lead_source\": \"paid_ads\",\n",
    "    \"number_of_courses_viewed\": 2,\n",
    "    \"annual_income\": 79276.0\n",
    "}\n",
    "\n",
    "prediction = pipeline.predict_proba(record)\n",
    "prediction[:, 1].round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb85f133",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Now let's serve this model as a web service\n",
    "\n",
    "* Install FastAPI\n",
    "* Write FastAPI code for serving the model\n",
    "* Now score this client using `requests`:\n",
    "\n",
    "```python\n",
    "url = \"YOUR_URL\"\n",
    "client = {\n",
    "    \"lead_source\": \"organic_search\",\n",
    "    \"number_of_courses_viewed\": 4,\n",
    "    \"annual_income\": 80304.0\n",
    "}\n",
    "requests.post(url, json=client).json()\n",
    "```\n",
    "\n",
    "What's the probability that this client will get a subscription?\n",
    "\n",
    "* 0.334\n",
    "* 0.534\n",
    "* 0.734\n",
    "* 0.934"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16f44462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Answer 4 :  {'prediction': 0.534}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "url = \"http://localhost:9696/predict\"\n",
    "\n",
    "client = {\n",
    "    \"lead_source\": \"organic_search\",\n",
    "    \"number_of_courses_viewed\": 4,\n",
    "    \"annual_income\": 80304.0\n",
    "}\n",
    "response = requests.post(url, json=client).json()\n",
    "print(\"## Answer 4 : \", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a09a88",
   "metadata": {},
   "source": [
    "## Docker\n",
    "\n",
    "Install [Docker](https://github.com/DataTalksClub/machine-learning-zoomcamp/blob/master/05-deployment/06-docker.md). \n",
    "We will use it for the next two questions.\n",
    "\n",
    "For these questions, we prepared a base image: `agrigorev/zoomcamp-model:2025`. \n",
    "You'll need to use it (see Question 5 for an example).\n",
    "\n",
    "This image is based on `3.13.5-slim-bookworm` and has\n",
    "a pipeline with logistic regression (a different one)\n",
    "as well a dictionary vectorizer inside. \n",
    "\n",
    "This is how the Dockerfile for this image looks like:\n",
    "\n",
    "```docker \n",
    "FROM python:3.13.5-slim-bookworm\n",
    "WORKDIR /code\n",
    "COPY pipeline_v2.bin .\n",
    "```\n",
    "\n",
    "We already built it and then pushed it to [`agrigorev/zoomcamp-model:2025`](https://hub.docker.com/r/agrigorev/zoomcamp-model).\n",
    "\n",
    "> **Note**: You don't need to build this docker image, it's just for your reference.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be68dca9",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Download the base image `agrigorev/zoomcamp-model:2025`. You can easily make it by using [docker pull](https://docs.docker.com/engine/reference/commandline/pull/) command.\n",
    "\n",
    "So what's the size of this base image?\n",
    "\n",
    "* 45 MB\n",
    "* 121 MB\n",
    "* 245 MB\n",
    "* 330 MB\n",
    "\n",
    "You can get this information when running `docker images` - it'll be in the \"SIZE\" column.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2f864b",
   "metadata": {},
   "source": [
    "## Answer 5\n",
    "121 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a1c724",
   "metadata": {},
   "source": [
    "## Dockerfile\n",
    "\n",
    "Now create your own `Dockerfile` based on the image we prepared.\n",
    "\n",
    "It should start like that:\n",
    "\n",
    "```docker\n",
    "FROM agrigorev/zoomcamp-model:2025\n",
    "# add your stuff here\n",
    "```\n",
    "\n",
    "Now complete it:\n",
    "\n",
    "* Install all the dependencies from pyproject.toml\n",
    "* Copy your FastAPI script\n",
    "* Run it with uvicorn \n",
    "\n",
    "After that, you can build your docker image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c980b21",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Let's run your docker container!\n",
    "\n",
    "After running it, score this client once again:\n",
    "\n",
    "```python\n",
    "url = \"YOUR_URL\"\n",
    "client = {\n",
    "    \"lead_source\": \"organic_search\",\n",
    "    \"number_of_courses_viewed\": 4,\n",
    "    \"annual_income\": 80304.0\n",
    "}\n",
    "requests.post(url, json=client).json()\n",
    "```\n",
    "\n",
    "What's the probability that this lead will convert?\n",
    "\n",
    "* 0.39\n",
    "* 0.59\n",
    "* 0.79\n",
    "* 0.99\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "061c0f56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Answer 6 :  {'prediction': 0.534}\n"
     ]
    }
   ],
   "source": [
    "url = \"http://localhost:9696/predict\"\n",
    "client = {\n",
    "    \"lead_source\": \"organic_search\",\n",
    "    \"number_of_courses_viewed\": 4,\n",
    "    \"annual_income\": 80304.0\n",
    "}\n",
    "response = requests.post(url, json=client).json()\n",
    "\n",
    "print(\"## Answer 6 : \", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3613f",
   "metadata": {},
   "source": [
    "## Submit the results\n",
    "\n",
    "* Submit your results here: https://courses.datatalks.club/ml-zoomcamp-2025/homework/hw05\n",
    "* If your answer doesn't match options exactly, select the closest one. If the answer is exactly in between two options, select the higher value.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d36e61f",
   "metadata": {},
   "source": [
    "## Publishing to Docker hub\n",
    "\n",
    "This is just for reference, this is how we published an image to Docker hub.\n",
    "\n",
    "`Dockerfile_base`: \n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.13.5-slim-bookworm\n",
    "WORKDIR /code\n",
    "COPY pipeline_v2.bin .\n",
    "```\n",
    "\n",
    "Publishing:\n",
    "\n",
    "```bash\n",
    "docker build -t mlzoomcamp2025_hw5 -f Dockerfile_base .\n",
    "docker tag mlzoomcamp2025_hw5:latest agrigorev/zoomcamp-model:2025\n",
    "docker push agrigorev/zoomcamp-model:2025\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
